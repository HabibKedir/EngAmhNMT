# -*- coding: utf-8 -*-
"""AmhNormToke.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1krBMqnUNOJ-4BRDdGmg5FZnE5mRP66l7

Python Code Implementation
"""

!pip install tensorflow transformers

"""Amharic Text Normalization and Customized Tokenizer"""

import re
import unicodedata
from collections import Counter

# Load Amharic data from Colab
def load_amharic_data(file_path):
    with open(file_path, "r", encoding="utf-8") as file:
        sentences = file.readlines()
    sentences = [line.strip() for line in sentences if line.strip()]
    return sentences

# Normalize Amharic text
def normalize_amharic_text(text):
    # Normalize Unicode
    text = unicodedata.normalize("NFC", text)
    # Remove diacritics
    text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')
    # Remove extra spaces and punctuation (if needed)
    text = re.sub(r"[፡።፣፤፥፦፧]", " ", text)  # Replace Amharic punctuation with space
    text = re.sub(r"\s+", " ", text).strip()  # Remove extra spaces
    return text

# Tokenizer for Amharic text
class AmharicTokenizer:
    def __init__(self, vocab=None):
        self.vocab = vocab or {}

    def fit(self, sentences):
        # Build vocabulary from Amharic sentences
        word_counter = Counter()
        for sentence in sentences:
            words = sentence.split()
            word_counter.update(words)
        self.vocab = {word: idx for idx, (word, _) in enumerate(word_counter.most_common(), start=1)}
        self.vocab["[PAD]"] = 0  # Add special token for padding

    def tokenize(self, text):
        # Tokenize text into word IDs
        words = text.split()
        tokens = [self.vocab.get(word, self.vocab.get("[UNK]", -1)) for word in words]
        return tokens

    def detokenize(self, tokens):
        # Convert tokens back to text
        inv_vocab = {idx: word for word, idx in self.vocab.items()}
        words = [inv_vocab.get(token, "[UNK]") for token in tokens]
        return " ".join(words)

# Main script
def main():
    # File path
    file_path = "/content/drive/MyDrive/New/amharic.txt"

    # Load and preprocess data
    sentences = load_amharic_data(file_path)
    print(f"Original Sentences:\n{sentences[:5]}")

    normalized_sentences = [normalize_amharic_text(sentence) for sentence in sentences]
    print(f"Normalized Sentences:\n{normalized_sentences[:5]}")

    # Initialize and train the tokenizer
    tokenizer = AmharicTokenizer()
    tokenizer.fit(normalized_sentences)

    # Tokenize the sentences
    tokenized_sentences = [tokenizer.tokenize(sentence) for sentence in normalized_sentences]
    print(f"Tokenized Sentences:\n{tokenized_sentences[:5]}")

    # Detokenize (reverse process)
    detokenized_sentences = [tokenizer.detokenize(tokens) for tokens in tokenized_sentences]
    print(f"Detokenized Sentences:\n{detokenized_sentences[:5]}")

    # Save the vocabulary (optional)
    with open("/content/drive/MyDrive/New/Ammharic.txt", "w", encoding="utf-8") as vocab_file:
        for word, idx in tokenizer.vocab.items():
            vocab_file.write(f"{word}\t{idx}\n")
    print("Vocabulary saved to /content/drive/MyDrive/New/Ammharic.txt")

if __name__ == "__main__":
    main()

